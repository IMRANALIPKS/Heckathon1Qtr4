"use strict";(self.webpackChunkhumanoid_robotics_book_ai=self.webpackChunkhumanoid_robotics_book_ai||[]).push([[110],{6072:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"vla","title":"Module 4 \u2014 Vision-Language-Action (VLA)","description":"Combining LLMs with perception and robot actions.","source":"@site/textbook/docs/05-vla.md","sourceDirName":".","slug":"/docs/vla","permalink":"/docs/vla","draft":false,"unlisted":false,"editUrl":"https://github.com/SUMAIRA-SHAIKH/Humanoid-Robotics-book-AI/textbook/docs/05-vla.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"vla","title":"Module 4 \u2014 Vision-Language-Action (VLA)","slug":"/docs/vla","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 \u2014 The AI-Robot Brain (NVIDIA Isaac)","permalink":"/docs/isaac"},"next":{"title":"Capstone \u2014 The Autonomous Humanoid","permalink":"/docs/capstone"}}');var s=i(4848),t=i(8453);const a={id:"vla",title:"Module 4 \u2014 Vision-Language-Action (VLA)",slug:"/docs/vla",sidebar_position:5},l="Vision-Language-Action (VLA)",r={},c=[{value:"Vision-Language Models",id:"vision-language-models",level:2},{value:"VLA Systems",id:"vla-systems",level:2},{value:"Challenges",id:"challenges",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"})}),"\n",(0,s.jsx)(e.p,{children:"Combining LLMs with perception and robot actions."}),"\n",(0,s.jsx)(e.h2,{id:"vision-language-models",children:"Vision-Language Models"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Grounding: Connecting language to visual elements"}),"\n",(0,s.jsx)(e.li,{children:"Reasoning: Inferring from visual and textual inputs"}),"\n",(0,s.jsx)(e.li,{children:"Planning: Generating action sequences from instructions"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"vla-systems",children:"VLA Systems"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"RT-1: Robotics Transformer 1"}),"\n",(0,s.jsx)(e.li,{children:"RT-2: Robotics Transformer 2 with vision-language models"}),"\n",(0,s.jsx)(e.li,{children:"Mobile ALOHA: Learning bimanual mobile manipulation"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"challenges",children:"Challenges"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Embodiment: Translating language to physical actions"}),"\n",(0,s.jsx)(e.li,{children:"Safety: Ensuring safe robot behavior"}),"\n",(0,s.jsx)(e.li,{children:"Generalization: Performing unseen tasks"}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>l});var o=i(6540);const s={},t=o.createContext(s);function a(n){const e=o.useContext(t);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),o.createElement(t.Provider,{value:e},n.children)}}}]);